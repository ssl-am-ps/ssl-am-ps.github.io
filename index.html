<!DOCTYPE html>
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Seamless Language Expansion: Enhancing multilingual mastery in self-supervised model for speech re-synthesis task</title>
  <style>
body {
  font-size: 20px;
}
audio {
  width: 200px;
  margin-right: 10px;
}

</style>
</head>
<body>
    <h2>Seamless Language Expansion: Enhancing multilingual mastery in self-supervised model for speech re-synthesis task</h2>
    
  <hr>

    <h2>Abstract</h2>
    <li>Self-supervised (SSL) models have shown great performance in various downstream tasks like speech recognition and synthesis. However, they are typically developed for a limited set of languages, and may encounter new languages when applied in real-world scenarios. Developing a SSL model from scratch for a new language is costly due to its huge parameters. Thus, it is vital to figure out how to efficiently adapt an existed SSL model to a new language without impairing its original abilities. We propose novel adaptation methods and preservation strategies to achieve this goal. Applied to mHuBERT, we investigate their effectiveness on multi-lingual speech re-synthesis task. Experiments show that adaptation methods enable mHuBERT to be applied to a new language (Mandarin) with the relative value of WER on speech re-synthesis task reduced up to 61.72%. Also, our preservation strategies ensure that the performance on speech re-synthesis for both existed and new languages remains intact. </li>

  <hr>
   


</body>
</html>
